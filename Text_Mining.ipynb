{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb039f95",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f358511",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "\n",
    "# Une fonction pour l'extraction du texte dans le pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    " \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Erreur : Le fichier '{pdf_path}' n'existe pas. Vérifiez le chemin.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction du texte du PDF : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "pdf_file_path = \"article_VO.pdf\"\n",
    "\n",
    "article_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "if article_text:\n",
    "    print(f\"Texte extrait :\\n{'-'*30}\\n{article_text[:500]}...\\n{'-'*30}\")\n",
    "else:\n",
    "    print(\"Échec de l'extraction du texte. Le nuage de mots ne pourra pas être généré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75549204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important de télecharger le package de mots de la librairie scapy\n",
    "#  python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dab89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# Charger le modèle spaCy pour le français\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Modèle spaCy 'fr_core_news_sm' non trouvé. Veuillez l'installer avec : python -m spacy download fr_core_news_sm\")\n",
    "    exit() # Quitte le script si le modèle n'est pas là\n",
    "\n",
    "def preprocess_text_for_wordcloud(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Prétraite le texte pour un nuage de mots pertinent en français.\n",
    "    Mise en minuscules, suppression de la ponctuation, des nombres, des mots vides et lemmatisation.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower()) # Convertir en minuscules et traiter\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Filtrer :\n",
    "        # - is_punct : Ponctuation\n",
    "        # - is_space : Espaces, retours chariot\n",
    "        # - is_stop : Mots vides (articles, prépositions, conjonctions, etc.)\n",
    "        # - is_alpha : S'assurer que c'est bien une lettre (pas un nombre ou un symbole après filtrage)\n",
    "        # - len(token.lemma_) > 1 : Éliminer les caractères uniques restants\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop and token.is_alpha and len(token.lemma_) > 1:\n",
    "            tokens.append(token.lemma_) # Utiliser la lemmatisation\n",
    "\n",
    "    return tokens\n",
    "\n",
    "if article_text:\n",
    "    processed_tokens = preprocess_text_for_wordcloud(article_text)\n",
    "    print(f\"\\nTokens prétraités (aperçu des 50 premiers) :\\n{processed_tokens[:50]}...\")\n",
    "\n",
    "    # Calculer la fréquence des mots après prétraitement\n",
    "    word_counts = Counter(processed_tokens)\n",
    "    print(f\"\\n20 mots les plus fréquents après prétraitement :\\n{word_counts.most_common(20)}\")\n",
    "else:\n",
    "    print(\"Le texte n'a pas pu être prétraité car il n'a pas été extrait du PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83473df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_tokens:\n",
    "    text_for_wordcloud = \" \".join(processed_tokens)\n",
    "\n",
    "    # Configurer le WordCloud\n",
    "    # 'width' et 'height' : Taille de l'image\n",
    "    # 'background_color' : Couleur de fond\n",
    "    # 'max_words' : Nombre maximal de mots à afficher\n",
    "    # 'min_font_size' : Taille minimale des polices\n",
    "    # 'collocations=False' : Important pour éviter que WordCloud ne regroupe des mots comme \"pomme de\" s'ils ne sont pas un concept unique\n",
    "    # 'stopwords' : Même si déjà filtrés, on peut les repasser pour être sûr (WordCloud a aussi sa propre liste)\n",
    "    #               Pour WordCloud, il faut une liste, pas un set.\n",
    "    french_stopwords = spacy.lang.fr.stop_words.STOP_WORDS # Utilise la liste de mots vides de spaCy\n",
    "\n",
    "    wordcloud = WordCloud(width=1200, height=600,\n",
    "                          background_color='white',\n",
    "                          max_words=100, # Afficher les 100 mots les plus pertinents\n",
    "                          min_font_size=10,\n",
    "                          collocations=False,\n",
    "                          stopwords=french_stopwords # Appliquer les stopwords ici aussi\n",
    "                         ).generate(text_for_wordcloud) # Utiliser la chaîne de tokens prétraités\n",
    "\n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off') # Cache les axes\n",
    "    plt.title(\"Nuage de mots pertinents de l'article de presse\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Impossible de générer le nuage de mots car aucun token pertinent n'a été extrait.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49077fb5",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyPDF2 nltk wordcloud matplotlib\n",
    "!{sys.executable} -m pip install stanza\n",
    "import nltk\n",
    "\n",
    "# Téléchargement des ressources NLTK (une seule fois suffit)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stanza.download(\"fr\")\n",
    "nlp = stanza.Pipeline(\n",
    "    lang=\"fr\",\n",
    "    processors=\"tokenize,mwt,pos,lemma\",\n",
    "    use_gpu=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire le texte\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"Le fichier '{pdf_path}' est introuvable.\")\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    return \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "article_text = extract_text_from_pdf(\"article_VO.pdf\")\n",
    "\n",
    "# Préparer les listes de filtrage\n",
    "french_stops = set(stopwords.words(\"french\"))\n",
    "custom_exclude = {\"soi\", \"leboncoin\"}  # mots spécifiques à retirer\n",
    "\n",
    "# Lemmatisation + filtrage\n",
    "doc = nlp(article_text)\n",
    "lemmas = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        lemma = word.lemma.lower()\n",
    "        if (\n",
    "            word.upos not in (\"PUNCT\", \"SYM\", \"NUM\", \"X\", \"VERB\")  # on enlève aussi les verbes\n",
    "            and len(lemma) > 1\n",
    "            and lemma not in french_stops\n",
    "            and lemma not in custom_exclude\n",
    "        ):\n",
    "            lemmas.append(lemma)\n",
    "\n",
    "# Aperçu et top 20\n",
    "counts = Counter(lemmas)\n",
    "print(\"20 lemmes les plus fréquents après filtration :\")\n",
    "for l, c in counts.most_common(20):\n",
    "    print(f\"{l:12s} → {c}\")\n",
    "\n",
    "# Générer le nuage de mots\n",
    "if lemmas:\n",
    "    wc = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color=\"white\",\n",
    "        max_words=100,\n",
    "        min_font_size=10,\n",
    "        collocations=False,\n",
    "        stopwords=french_stops.union(custom_exclude)\n",
    "    ).generate(\" \".join(lemmas))\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Nuage de mots\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Aucun lemme pertinent après filtration.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
